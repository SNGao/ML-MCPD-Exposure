---
title: "02_Model_Predictions"
---
# 01.Load Packages
```{r}
library(modelr)
library(mice)
library(VIM)
library(DMwR2)
library(rpart)
library(car)
library(splines)
library(gam)
library(caret)
library(ParBayesianOptimization)


source('02_src/defined_functions.R')
data_MCPD = read.csv('00_Data/process_data.csv') # Generated from STATA files
data_MCPD$X = NULL
```


# 02.Partition Train and Test Data
```{r}
# Divide the data into training sets and test sets
df = data_MCPD
set.seed(123)
train_index <- createDataPartition(df$dietaryMCPD, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Prepare for data
X_train <- train_data[, -which(names(train_data) == "dietaryMCPD")]
y_train <- train_data[1]
X_test <- test_data[, -which(names(test_data) == "dietaryMCPD")]
y_test <- test_data[1]
```

# 03.Model Training and Test
## 3.1-MLR
```{r}
model.mlr <- lm(dietaryMCPD~., data = fold.train)
```


## 3.2-GAM
### Train Model and Test
```{r}

```



## 3.3-SVR
### Paramater Grid for Search
```{r}
library(e1071)
param_grid_svr <- list(
  cost = c(0.1, 1, 10, 100),       # penalty parameters
  gamma = c(0.001, 0.01, 0.1, 1),  # parameter of kernal functions
  epsilon = c(0.01, 0.1, 0.2, 0.3) # loose variable in loss functions
)
```

### Random Grid Search
```{r, warning=FALSE}
best_params_svr <- list()
best_rmse_svr <- Inf
ctrl <- trainControl(method = "cv", number = 5)

for (i in 1:100) {  # random parameter combinations
  params <- lapply(param_grid_svr, function(x) sample(x, 1))
  
  svr_model <- train(
    x = X_train,
    y = y_train$dietaryMCPD,
    method = "svmRadial",
    tuneGrid = expand.grid(
      C = params$cost,
      sigma = 1 / (ncol(X_train) * params$gamma)  # caret's SVM requires sigma instead of gamma
    ),
    trControl = ctrl,
    epsilon = params$epsilon,
    metric = "RMSE"
  )
  
  rmse <- min(svr_model$results$RMSE)
  
  if (rmse < best_rmse_svr) {
    best_params_svr <- params
    best_rmse_svr <- rmse
  }
}
print(best_rmse_svr^2)
```

### Model Performance
```{r}
final_svr_model <- svm(
  x = X_train,
  y = y_train$dietaryMCPD,
  cost = best_params_svr$cost,
  gamma = 1 / (ncol(X_train) * best_params_svr$gamma) ,
  epsilon = best_params_svr$epsilon,
  type = 'eps-regression'
)

# Make predictions on the training set
train_pred_svr <- predict(final_svr_model, X_train)
train_perf_svr <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_svr))

# Make predictions on the test set
test_pred_svr <- predict(final_svr_model, X_test)
test_perf_svr <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_svr))

print(test_perf_svr)
```


## 3.4-RandomForest
### Paramater Grid for Search
```{r}
library(randomForest)
param_grid_rf <- list(
  mtry = c(2, 4, 6, 8, 10),         # Number of randomly selected features when splitting
  ntree = c(100, 200, 300, 400, 500), # The number of decision trees
  maxnodes = c(10, 20, 30, 40, 50),  # Maximum number of nodes per tree
  nodesize = c(1, 5, 10)             # Minimum number of samples per node
)
```

### Random Grid Search
```{r}
set.seed(1)

best_params_rf <- list()
best_rmse_rf <- Inf
ctrl <- trainControl(method = "cv", number = 5)

for (i in 1:10) {  # random parameter combinations
  params <- lapply(param_grid_rf, function(x) sample(x, 1))
  
  rf_model <- train(
    x = X_train,
    y = y_train$dietaryMCPD,
    method = "rf",
    tuneGrid = expand.grid(mtry = params$mtry),
    trControl = ctrl,
    ntree = params$ntree,
    maxnodes = params$maxnodes,
    nodesize = params$nodesize,
    metric = "RMSE"
  )
  
  rmse <- min(rf_model$results$RMSE)
  
  if (rmse < best_rmse_rf) {
    best_params_rf <- params
    best_rmse_rf <- rmse
  }
}

print(list(best_rmse_rf^2))
```

### Model Performance
```{r}
# Train the model with the best parameters
final_rf_model <- randomForest(
  x = X_train,
  y = y_train$dietaryMCPD,
  mtry = best_params_rf$mtry,
  ntree = best_params_rf$ntree,
  maxnodes = best_params_rf$maxnodes,
  nodesize = best_params_rf$nodesize,
  importance = TRUE
)

# Make predictions on the training set
train_pred_rf <- predict(final_rf_model, X_train)
train_perf_rf <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_rf))

# Make predictions on the test set
test_pred_rf <- predict(final_rf_model, X_test)
test_perf_rf <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_rf))

print(test_perf_rf)
```

## 3.5-XGBoost
### Paramater Grid for Search
```{r}
library(xgboost)
param_grid <- list(
  eta = c(0.01, 0.05, 0.1, 0.2),
  max_depth = c(4, 6, 8, 10),
  nrounds = c(100, 200, 300, 400, 500),
  subsample = c(0.6, 0.7, 0.8, 0.9, 1.0),
  colsample_bytree = c(0.6, 0.7, 0.8, 0.9, 1.0),
  lambda = c(0, 0.1, 0.5, 1.0),
  alpha = c(0, 0.1, 0.5, 1.0)
)
```

### Random Grid Search
```{r}
set.seed(1)
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))

# Random search by cross-validation
best_params_xgb <- list()
best_rmse_xgb = Inf

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- xgb.cv(
    data = dtrain,
    params = params,
    nrounds = params$nrounds,
    early_stopping_rounds = 10,
    nfold = 5,
    verbose = 0,
  )
  rmse <- min(cv_results$evaluation_log$test_rmse_mean)
  
  if (rmse < best_rmse_xgb) {
    best_param_xgb <- params
    best_rmse_xgb <- rmse
    best_iter_xgb <- cv_results$best_iteration
  }
}
```

### Model Performance
```{r}
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))
xgb.model <- xgb.train(data = dtrain,
                  params = best_param_xgb,
                  nrounds = best_iter_xgb,
                  verbose = 1
) 

# Predict performance
train_pred_xgb <- predict(xgb.model, dtrain)
train_perf_xgb = defaultSummary(data.frame(obs = y_train$dietaryMCPD,
                                           pred = train_pred_xgb))
  
# Predict performance in Test Sets
test_pred_xgb <- predict(xgb.model, dtest)
test_perf_xgb = defaultSummary(data.frame(obs = y_test$dietaryMCPD, 
                                          pred = test_pred_xgb))

print(test_perf_xgb)
```



## 3.6-LGBM
### Paramater Grid for Search
```{r}
library(lightgbm)
param_grid <- list(
  objective = 'regression_l2',
  metric = 'mse', # l2: corresponds to MSE
  num_leaves = c(10, 20, 30, 40, 50),
  learning_rate = c(0.01, 0.05, 0.1, 0.2),
  max_depth = c(-1, 5, 10, 15),
  feature_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_freq = c(1, 5, 10),
  lambda_l1 = c(0, 0.1, 0.5, 1.0),
  lambda_l2 = c(0, 0.1, 0.5, 1.0)
)
```

### Random Grid Search
```{r}
set.seed(1)
# Create a lgb.Dataset object for train Model
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

# Random search by cross-validation
best_param_lgb <- list()
best_mse = Inf
best_iter = 0

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- lgb.cv(
    data = train_data,
    params = params,
    nrounds = 1000,
    early_stopping_rounds = 10,
    nfold = 5,
    verbose = -1,
  )
  mse <- cv_results$record_evals$valid$l2$eval[[cv_results$best_iter]]
  
  if (mse < best_mse) {
    best_param_lgb <- params
    best_mse <- mse
    best_iter <- cv_results$best_iter
  }
}

print(list(best_mse, best_iter))
```

### Model Performance
```{r}
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

lgb1 <- lgb.train(data = train_data,
                  params = best_param_lgb,
                  nrounds = best_iter,
                  verbose = 1,
                  obj = 'regression_l2'
) 

# Predict performance
train_pred_xgb <- predict(lgb1, as.matrix(X_train))
train_perf_xgb = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = trainpred))
  
# Predict performance in Test Sets
test_pred_xgb <- predict(lgb1, as.matrix(X_test))
test_perf_xgb = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = testpred))
print(test_perf_xgb)
```

## 3.7-Catboost
### Paramater Grid for Search
```{r}
set.seed(1)
library(catboost)

param_grid <- list(
  loss_function = 'RMSE',
  depth = c(4, 6, 8, 10),
  learning_rate = c(0.01, 0.05, 0.1, 0.2),
  iterations = c(100, 200, 300, 400, 500),
  l2_leaf_reg = c(1, 3, 5, 7, 9),
  border_count = c(32, 64, 128),
  bagging_temperature = c(0.1, 0.5, 1, 2, 5)
)
```

### Random Grid Search
```{r}
set.seed(208)
train_pool <- catboost.load_pool(data = X_train, label = y_train)

# Random search by cross-validation
best_params <- list()
best_rmse <- Inf

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  output <- capture.output({
      cv_results <- catboost.cv(
        pool = train_pool,
        params = params,
        fold_count = 5,
        early_stopping_rounds = 10
      )
  })
  
  rmse <- min(cv_results$test.RMSE.mean)
  print(rmse)
  
  if (rmse < best_rmse) {
    best_params <- params
    best_rmse <- rmse
  }
}
best_param.cat = best_params
print(best_rmse^2) # print MSE
```

### Model Performance
```{r}
test_pool <- catboost.load_pool(data = X_test, label = y_test)
cat_model <- catboost.train(train_pool,
                            params = best_param.cat)

# Predict performance
train_pred_cat <- catboost.predict(cat_model, train_pool)
train_perf_cat = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = train_pred_cat))
  
# Predict performance in Test Sets
test_pred_cat <- catboost.predict(cat_model, test_pool)
test_perf_cat = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_cat))
print(test_perf_cat)
```


# 04.Summarize Parameters
## Hyper-parameters in different Models
```{r}
hyper_para = list()
hyper_para[["svr"]] = best_params_svr
hyper_para[["rf"]] = best_params_rf
hyper_para[["xgb"]] = best_param.xgb
hyper_para[["lgb"]] = best_param.lgb
hyper_para[["cat"]] = best_param.cat
```

## Prediction performance in all models
```{r}

```

