---
title: "02_Model_Predictions"
---
# 01.Load Packages
```{r}
library(modelr)
library(mice)
library(VIM)
library(DMwR2)
library(rpart)
library(car)
library(splines)
library(gam)
library(caret)
library(ParBayesianOptimization)


source('defined_functions.R')
data_MCPD = read.csv('00_Data/process_data.csv') # Generated from STATA files
data_MCPD$X = NULL
```


# 02.Partition Train and Test Data
```{r}
# Divide the data into training sets and test sets
df = data_MCPD
set.seed(123)
train_index <- createDataPartition(df$dietaryMCPD, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Prepare for data
X_train <- train_data[, -which(names(train_data) == "dietaryMCPD")]
y_train <- train_data[1]
X_test <- test_data[, -which(names(test_data) == "dietaryMCPD")]
y_test <- test_data[1]
```

# 03.Model Training and Test
## 3.1-MLR

## 3.2-GAM

## 3.3-SVR

## 3.4-RandomForest

## 3.5-XGBoost
```{r}
library(xgboost)

```

## 3.6-LGBM
### Paramater Grid for Search
```{r}
library(lightgbm)
param_grid <- list(
  objective = 'regression_l2',
  metric = 'mse', # l2: corresponds to MSE
  num_leaves = c(10, 20, 30, 40, 50),
  learning_rate = c(0.01, 0.05, 0.1, 0.2),
  max_depth = c(-1, 5, 10, 15),
  feature_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_freq = c(1, 5, 10),
  lambda_l1 = c(0, 0.1, 0.5, 1.0),
  lambda_l2 = c(0, 0.1, 0.5, 1.0)
)
```

### Random Grid Search
```{r}
set.seed(208)
# Create a lgb.Dataset object for train Model
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

# Random search by cross-validation
best_params <- list()
best_mse = Inf
best_iter = 0

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- lgb.cv(
    data = train_data,
    params = params,
    nrounds = 1000,
    early_stopping_rounds = 10,
    nfold = 5,
    verbose = -1,
  )
  mse <- cv_results$record_evals$valid$l2$eval[[cv_results$best_iter]]
  
  if (mse < best_mse) {
    best_params <- params
    best_mse <- mse
    best_iter <- cv_results$best_iter
  }
}

print(list(best_mse, best_iter))
```

### Model Performance
```{r}
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

lgb1 <- lgb.train(data = train_data,
                  params = best_params,
                  nrounds = best_iter,
                  verbose = 1,
                  obj = 'regression_l2'
) 

# Predict performance
trainpred <- predict(lgb1, as.matrix(X_train))
temp.train = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = trainpred))
  
# Predict performance in Test Sets
testpred <- predict(lgb1, as.matrix(X_test))
temp.test = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = testpred))
```

## 3.7-Catboost
### Paramater Grid for Search
```{r}
library(catboost)

param_grid <- list(
  loss_function = 'RMSE',
  depth = c(4, 6, 8, 10),
  learning_rate = c(0.01, 0.05, 0.1, 0.2),
  iterations = c(100, 200, 300, 400, 500),
  l2_leaf_reg = c(1, 3, 5, 7, 9),
  border_count = c(32, 64, 128),
  bagging_temperature = c(0.1, 0.5, 1, 2, 5)
)
```

### Random Grid Search
```{r}
set.seed(208)
train_pool <- catboost.load_pool(data = X_train, label = y_train)

# Random search by cross-validation
best_params <- list()
best_mse <- Inf

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- catboost.cv(
    pool = train_pool,
    params = params,
    early_stopping_rounds = 10,
    fold_count = 5
  )
  mse <- min(cv_results$test.RMSE.mean)
  
  if (mse < best_mse) {
    best_params <- params
    best_mse <- mse
  }
}
```

### Model Performance
```{r}
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

lgb1 <- lgb.train(data = train_data,
                  params = best_params,
                  nrounds = best_iter,
                  verbose = 1,
                  obj = 'regression_l2'
) 

# Predict performance
trainpred <- predict(lgb1, as.matrix(X_train))
temp.train = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = trainpred))
  
# Predict performance in Test Sets
testpred <- predict(lgb1, as.matrix(X_test))
temp.test = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = testpred))
```


