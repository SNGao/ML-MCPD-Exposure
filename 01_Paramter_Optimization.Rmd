---
title: "02_Model_Predictions"
---
# 01.Load Packages
```{r, warning=FALSE}
library(modelr)
library(mice)
library(VIM)
library(DMwR2)
library(rpart)
library(car)
library(splines)
library(gam)
library(caret)
library(ParBayesianOptimization)

source('02_src/defined_functions.R')
data_MCPD = read.csv('00_Data/process_data.csv') # Generated from STATA files
data_MCPD$X = NULL
data_MCPD$WEIGHT = NULL

# str(data_MCPD)
# Define Catogory Variables
data_MCPD$SEX<-as.factor(data_MCPD$SEX)
data_MCPD$FAMINCOM5w<-as.factor(data_MCPD$FAMINCOM5w)
data_MCPD$SMOKE<-as.factor(data_MCPD$SMOKE)
data_MCPD$MARRIAGE<-as.factor(data_MCPD$MARRIAGE)
data_MCPD$education<-as.factor(data_MCPD$education)
data_MCPD$alcohol<-as.factor(data_MCPD$alcohol)
data_MCPD$CVD<-as.factor(data_MCPD$CVD)
data_MCPD$db<-as.factor(data_MCPD$db)
data_MCPD$HBP<-as.factor(data_MCPD$HBP)

cat_list = c('SEX', 'FAMINCOM5w', 'SMOKE', 'MARRIAGE', 'education', 'alcohol', 'CVD', 'db', 'HBP')
```


# 02.Partition Train and Test Data
```{r, warning=FALSE}
# Divide the data into training sets and test sets
df = data_MCPD
set.seed(0612) #0, not 110, 考虑20240606, 不考虑20240616; 0610-0611, 很好20240610，但需要修改RF
train_index <- createDataPartition(df$dietaryMCPD, p = 0.7, list = FALSE)
train_data <- df[train_index, ]
test_data <- df[-train_index, ]

# Prepare for data
X_train <- train_data[, -which(names(train_data) == "dietaryMCPD")]
y_train <- train_data[1]
X_test <- test_data[, -which(names(test_data) == "dietaryMCPD")]
y_test <- test_data[1]
```

# Check Distributions of Train and Test Sets
## K-S Test
```{r, warning=FALSE}
hist(y_train$dietaryMCPD)
hist(y_test$dietaryMCPD)

ks.test(y_train$dietaryMCPD, y_test$dietaryMCPD)
```

## Table for Train and Test Set
```{r, warning=FALSE}
data.raw = read.csv('00_Data/data_MCPD.csv') # Generated from STATA files
data.raw$MARRIAGE = ifelse(data.raw$MARRIAGE == 1, 3, data.raw$MARRIAGE)
dat.table = data.raw[row.names(data_MCPD),]
dat.table$Label = NA
dat.table$Label[train_index] = 'Train'
dat.table$Label[-train_index] = 'Test'

colnames(dat.table)
## Transform Variable Name
dat.table = dat.table |>
  rename(Age = age,
         DHPMA = DHPMA_a,
         Smoking = SMOKE,
         Income = FAMINCOM5w,
         DB = db,
         Married = MARRIAGE,
         Drinking = alcohol,
         Physical_Activity = MET) |>
  mutate(SEX = factor(
      SEX,
      levels = c(1, 2),
      labels = c("Men", "Women")
    ))



dat.table %>%
  select(Age, education, Married, SEX,Label,
         Income, Smoking, Drinking,
         CVD, DB, HBP, Physical_Activity,
         BMI, DHPMA, dietaryMCPD,
         BEANOILintake, PEANUTOIintake, LARDOILintake, totalenergy) %>%
  tbl_summary(
    by = "Label",
    statistic = list(
      all_continuous() ~ "{mean} ({sd})",
      all_categorical() ~ "{n} ({p}%)"
    ),
    digits = list(
      all_continuous() ~ c(2, 2),  # 保留两位小数
      all_categorical() ~ c(0, 2)  # 保留两位小数的百分比
    )) %>%
  add_overall() %>%
  add_p(test = list(all_continuous() ~ "aov",
                    all_categorical() ~ "chisq.test")) %>%
  as_flex_table() %>%
  save_as_docx(path = "01_Output_Results/Table23-Train-Test.docx")
```


# 03.Model Training and Test
In this section, we will not perform parameter optimization for multiple linear regression (MLR) and generalized additive models (GAM). This decision is based on prior variable selection conducted in an earlier analysis, where no signs of overfitting were detected. For more complex models, including Random Forest, Support Vector Regression (SVR), XGBoost, LightGBM, and CatBoost, we will employ a random grid search approach (N=100) to optimize their respective hyperparameters. This is necessary due to the intricate nature of their model settings and the need to fine-tune multiple parameters to enhance model performance.

## 3.1-MLR
### Model Performance
```{r, warning=FALSE}
data_MCPD$WEIGHT = NULL
model.mlr <- lm(dietaryMCPD ~ age+SEX+education+MARRIAGE+alcohol+SMOKE
                +FAMINCOM5w
                +HEIGHT+BMI+CREATININE+MET+CVD+HBP+MET+db+HBP
                +totalenergy+BEANOILintake+PEANUTOIintake+LARDOILintake
                +DHPMA_a,
                data = train_data)
# summary(model.mlr)

# Make predictions on the training set
train_pred_mlr <- predict(model.mlr, X_train)
train_perf_mlr <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_mlr))

# Make predictions on the test set
test_pred_mlr <- predict(model.mlr, X_test)
test_perf_mlr <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_mlr))

print(test_perf_mlr)
```


## 3.2-GAM
### Train Model and Test
```{r, warning=FALSE}
model.gam <- lm(dietaryMCPD~CREATININE+HEIGHT+BMI+bs(age)
                             + bs(BEANOILintake) + bs(PEANUTOIintake) + bs(LARDOILintake) 
                             + MET + DHPMA_a + bs(totalenergy)
                             + MARRIAGE + SEX + FAMINCOM5w + CVD + SMOKE        
                             + HBP+alcohol+education
                             , data = train_data)
a = summary(model.gam)
# Make predictions on the training set
train_pred_gam <- predict(model.gam, train_data)
train_perf_gam <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_gam))

# Make predictions on the test set
test_pred_gam <- predict(model.gam, test_data)
test_perf_gam <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_gam))

print(test_perf_gam)
```

### Transform factor to continuous
```{r, warning=FALSE}
cat_num = which(colnames(X_train) %in% cat_list )
X_train[,cat_num] = lapply(X_train[,cat_num], as.numeric)
X_test[,cat_num] = lapply(X_test[,cat_num], as.numeric)
```

## 3.3-SVR
### Paramater Grid for Search
```{r, warning=FALSE}
library(e1071)
param_grid_svr <- list(
  cost = c(0.1, 1, 10, 100),       # penalty parameters
  gamma = c(0.001, 0.01, 0.1, 1),  # parameter of kernal functions
  epsilon = c(0.01, 0.1, 0.2, 0.3) # loose variable in loss functions
)
```

### Random Grid Search
```{r, warning=FALSE}
set.seed(210)
best_params_svr <- list()
best_rmse_svr <- Inf
ctrl <- trainControl(method = "cv", number = 5)

for (i in 1:100) {  # random parameter combinations
  params <- lapply(param_grid_svr, function(x) sample(x, 1))
  
  svr_model <- train(
    x = X_train,
    y = y_train$dietaryMCPD,
    method = "svmRadial",
    tuneGrid = expand.grid(
      C = params$cost,
      sigma = 1 / (ncol(X_train) * params$gamma)  # caret's SVM requires sigma instead of gamma
    ),
    trControl = ctrl,
    epsilon = params$epsilon,
    metric = "RMSE"
  )
  
  rmse <- min(svr_model$results$RMSE)
  
  if (rmse < best_rmse_svr) {
    best_params_svr <- params
    best_rmse_svr <- rmse
  }
}
# print(best_rmse_svr^2)
```


### Model Performance
```{r, warning=FALSE}
final_svr_model <- svm(
  x = X_train,
  y = y_train$dietaryMCPD,
  cost = best_params_svr$cost,
  gamma = 1 / (ncol(X_train) * best_params_svr$gamma) ,
  epsilon = best_params_svr$epsilon,
  type = 'eps-regression'
)

# Make predictions on the training set
train_pred_svr <- predict(final_svr_model, X_train)
train_perf_svr <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_svr))

# Make predictions on the test set
test_pred_svr <- predict(final_svr_model, X_test)
test_perf_svr <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_svr))

print(test_perf_svr)
```



## 3.4-RandomForest
### Paramater Grid for Search
```{r, warning=FALSE}
library(randomForest)
param_grid_rf <- list(
  mtry = c(6, 8, 10),         # Number of randomly selected features when splitting
  ntree = c(500, 600, 700), # The number of decision trees
  maxnodes = c(60, 80, 100),  # Maximum number of nodes per tree
  nodesize = c(6, 8, 10)             # Minimum number of samples per node
)
```

### Random Grid Search
```{r, warning=FALSE}
set.seed(210)
best_params_rf <- list()
best_rmse_rf <- Inf
ctrl <- trainControl(method = "cv", number = 5)

for (i in 1:100) {  # random parameter combinations
  params <- lapply(param_grid_rf, function(x) sample(x, 1))
  
  rf_model <- train(
    x = X_train,
    y = y_train$dietaryMCPD,
    method = "rf",
    tuneGrid = expand.grid(mtry = params$mtry),
    trControl = ctrl,
    ntree = params$ntree,
    maxnodes = params$maxnodes,
    nodesize = params$nodesize,
    metric = "MAE"
  )
  
  rmse <- min(rf_model$results$RMSE)
  
  if (rmse < best_rmse_rf) {
    best_params_rf <- params
    best_rmse_rf <- rmse
  }
}

# print(list(best_rmse_rf^2))
```

### Model Performance
```{r, warning=FALSE}
# Train the model with the best parameters
# best_params_rf
set.seed(2)
final_rf_model <- randomForest(
  x = X_train,
  y = y_train$dietaryMCPD,
  mtry = best_params_rf$mtry, #8
  ntree = best_params_rf$ntree, #700
  maxnodes = best_params_rf$maxnodes, #80
  nodesize = best_params_rf$nodesize, #10
  importance = TRUE
)

# Make predictions on the training set
train_pred_rf <- predict(final_rf_model, X_train)
train_perf_rf <- defaultSummary(data.frame(obs = y_train$dietaryMCPD, pred = train_pred_rf))

# Make predictions on the test set
test_pred_rf <- predict(final_rf_model, X_test)
test_perf_rf <- defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_rf))

print(test_perf_rf)
# importance(final_rf_model)
# varImpPlot(final_rf_model)
```

## 3.5-XGBoost
### Paramater Grid for Search
```{r, warning=FALSE}
library(xgboost)
param_grid <- list(
  eta = c(0.05, 0.1, 0.2, 0.3),
  max_depth = c(2, 4, 6, 8),
  nrounds = c(200, 300, 400, 500),
  subsample = c(0.7, 0.8, 0.9, 1.0),
  colsample_bytree = c(0.5, 0.6, 0.7, 0.8),
  lambda = c(0.1, 0.3, 0.5),
  alpha = c(0.1, 0.3, 0.5)
)
```

### Random Grid Search
```{r, warning=FALSE}
set.seed(210)
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))

# Random search by cross-validation
best_param_xgb <- list()
best_rmse_xgb = Inf

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- xgb.cv(
    data = dtrain,
    params = params,
    nrounds = params$nrounds,
    early_stopping_rounds = 10,
    nfold = 5,
    verbose = 0,
  )
  rmse <- min(cv_results$evaluation_log$test_rmse_mean)
  
  if (rmse < best_rmse_xgb) {
    best_param_xgb <- params
    best_rmse_xgb <- rmse
    best_iter_xgb <- cv_results$best_iteration
  }
}
```

### Model Performance
```{r, warning=FALSE}
xgb.model <- xgb.train(data = dtrain,
                  params = best_param_xgb,
                  nrounds = best_iter_xgb,
                  verbose = 1
) 

# Predict performance
train_pred_xgb <- predict(xgb.model, dtrain)
train_perf_xgb = defaultSummary(data.frame(obs = y_train$dietaryMCPD,
                                           pred = train_pred_xgb))
   
# Predict performance in Test Sets
test_pred_xgb <- predict(xgb.model, dtest)
test_perf_xgb = defaultSummary(data.frame(obs = y_test$dietaryMCPD, 
                                          pred = test_pred_xgb))

print(test_perf_xgb)
XGB.VIP = data.frame(xgb.importance(model = xgb.model))
pdf('01_Output_Results/XGB_VIP.pdf', height = 8, width = 6)
xgb.plot.importance(importance_matrix = xgb.importance(model = xgb.model),
                    measure = 'Frequency')
xgb.plot.importance(importance_matrix = xgb.importance(model = xgb.model),
                    measure = 'Cover')
xgb.plot.importance(importance_matrix = xgb.importance(model = xgb.model),
                    measure = 'Gain')
dev.off()

write.csv(XGB.VIP, '01_Output_Results/XGB_VIP.csv')
```


## 3.6-LGBM
### Paramater Grid for Search
```{r, warning=FALSE}
library(lightgbm)
param_grid <- list(
  objective = 'regression_l2',
  metric = 'mse', # l2: corresponds to MSE
  num_leaves = c(10, 20, 30, 40, 50),
  learning_rate = c(0.1, 0.2, 0.3),
  max_depth = c(-1, 5, 10, 20),
  feature_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_fraction = c(0.6, 0.7, 0.8, 0.9, 1.0),
  bagging_freq = c(5, 10),
  lambda_l1 = c(0.1, 0.3, 0.5, 1.0),
  lambda_l2 = c(0.1, 0.3, 0.5, 1.0)
)
```

### Random Grid Search
```{r, warning=FALSE}
set.seed(210)
# Create a lgb.Dataset object for train Model
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

# Random search by cross-validation
best_param_lgb <- list()
best_mse_lgb = Inf
best_iter_lgb = 0

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  cv_results <- lgb.cv(
    data = train_data,
    params = params,
    nrounds = 1000,
    early_stopping_rounds = 10,
    nfold = 5,
    verbose = -1,
  )
  mse <- cv_results$record_evals$valid$l2$eval[[cv_results$best_iter]]
  
  if (mse < best_mse_lgb) {
    best_param_lgb <- params
    best_mse_lgb <- mse
    best_iter_lgb <- cv_results$best_iter
  }
}

print(list(best_mse_lgb, best_iter_lgb))
```

### Model Performance
```{r, warning=FALSE}
train_data <- lgb.Dataset(data = as.matrix(X_train), label = as.matrix(y_train))

lgb1 <- lgb.train(data = train_data,
                  params = best_param_lgb,
                  nrounds = best_iter_lgb,
                  verbose = -1,
                  obj = 'regression_l2'
) 

# Predict performance
train_pred_lgb <- predict(lgb1, as.matrix(X_train))
train_perf_lgb = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = train_pred_lgb))
  
# Predict performance in Test Sets
test_pred_lgb <- predict(lgb1, as.matrix(X_test))
test_perf_lgb = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_lgb))
print(test_perf_lgb)

```

## 3.7-Catboost
### Paramater Grid for Search
```{r, warning=FALSE}
library(catboost)

param_grid <- list(
  loss_function = 'RMSE',
  depth = c(2, 3, 4, 5),
  learning_rate = c(0.01, 0.03, 0.05, 0.1),
  iterations = c(200, 300, 400, 500),
  l2_leaf_reg = c(0.05, 0.1, 0.3, 0.5),
  border_count = c(52, 108, 216),
  bagging_temperature = c(0.05, 0.07, 0.1, 0.2)
)
```

### Random Grid Search
```{r, warning=FALSE}
set.seed(210)
train_pool <- catboost.load_pool(data = X_train, label = y_train)

# Random search by cross-validation
best_param_cat <- list()
best_rmse_cat <- Inf

for (i in 1:100) {  # Number of random parameter combinations to try
  params <- lapply(param_grid, function(x) sample(x, 1))
  
  output <- capture.output({
      cv_results <- catboost.cv(
        pool = train_pool,
        params = params,
        fold_count = 5,
        early_stopping_rounds = 10
      )
  }) # avoid print unrelated information
  
  rmse <- min(cv_results$test.RMSE.mean)
  
  if (rmse < best_rmse_cat) {
    best_param_cat <- params
    best_rmse_cat <- rmse
  }
}
print(best_rmse_cat^2) # print MSE
```

### Model Performance
```{r, warning=FALSE}
test_pool <- catboost.load_pool(data = X_test, label = y_test)

output <- capture.output({
  cat_model <- catboost.train(train_pool,
                              params = best_param_cat) 
}) # avoid print unrelated information

# Predict performance
train_pred_cat <- catboost.predict(cat_model, train_pool)
train_perf_cat = defaultSummary(data.frame(obs = y_train$dietaryMCPD,pred = train_pred_cat))
  
# Predict performance in Test Sets
test_pred_cat <- catboost.predict(cat_model, test_pool)
test_perf_cat = defaultSummary(data.frame(obs = y_test$dietaryMCPD, pred = test_pred_cat))
print(test_perf_cat)


```

# 04.Summarize Parameters
## Hyper-parameters in different Models
```{r, warning=FALSE}
hyper_para = list()
hyper_para[["svr"]] = best_params_svr
hyper_para[["rf"]] = best_params_rf
hyper_para[["xgb"]] = best_param_xgb
hyper_para[["lgb"]] = best_param_lgb
hyper_para[["cat"]] = best_param_cat

write.csv(hyper_para$svr, '01_Output_Results/Parameter.SVR.csv')
write.csv(hyper_para$rf, '01_Output_Results/Parameter.RF.csv')
write.csv(hyper_para$xgb, '01_Output_Results/Parameter.XGB.csv')
write.csv(hyper_para$lgb, '01_Output_Results/Parameter.LGB.csv')
write.csv(hyper_para$cat, '01_Output_Results/Parameter.CAT.csv')
```

## Prediction performance in all models
```{r, warning=FALSE}
result.combined = data.frame(
           test_perf_mlr,
           test_perf_gam,
           test_perf_svr,
           test_perf_rf,
           test_perf_xgb,
           test_perf_lgb,
           test_perf_cat)

colnames(result.combined) = c('MLR', 'GAM', 'SVR', 'RF', 'XGBoost', 'LightGBM', 'CatBoost')

# which(result.combined[1,] == min(result.combined[1,]))
# which(result.combined[2,] == max(result.combined[2,]))
# which(result.combined[3,] == min(result.combined[3,]))

result.combined
write.csv(result.combined, '01_Output_Results/TestSet_Performance.csv')
```
# 05.Variable Importance
```{r, warning=FALSE}
## Single Importance
VIP_cat = data.frame(cat_model$feature_importances)
colnames(VIP_cat) = 'FeatureImportance'
VIP_cat$Variable = row.names(VIP_cat)
VIP_cat = VIP_cat[order(VIP_cat$FeatureImportance,
              decreasing = TRUE),]
write.csv(VIP_cat, '01_Output_Results/CatBoost_VIP_Single.csv')

## Interaction Importance
VIP.tmp = data.frame(cat_model$feature_importances)
all_pool <- catboost.load_pool(data = data_MCPD[,-1], label = data_MCPD$dietaryMCPD)

VIP_I_cat = data.frame(catboost.get_feature_importance(cat_model, 
                                            test_pool, # test_pool, 
                                            type = 'Interaction'))
colnames(VIP_I_cat) = c('Variable-1', 'Variable-2', 'Score')
mapping_vector <- setNames(row.names(VIP.tmp), 1:length(row.names(VIP.tmp)))

VIP_I_cat$`Variable-1` <- mapping_vector[as.character(VIP_I_cat$`Variable-1`)]
VIP_I_cat$`Variable-2` <- mapping_vector[as.character(VIP_I_cat$`Variable-2`)]


### Calculate total score (%) of interaction effects
index_target = c(which(VIP_I_cat$`Variable-1` == 'DHPMA_a'),
                 which(VIP_I_cat$`Variable-2` == 'DHPMA_a'))
sum(VIP_I_cat$Score[index_target])
VIP_I_cat
write.csv(VIP_I_cat, '01_Output_Results/CatBoost_VIP_Interaction.csv')
```

# 06.Generate Visulization Plots
```{r}

pdf('01_Output_Results/Visulization.pdf', width = 8, height = 6)
ActualvsPredict(y_test, test_pred_mlr, 'MLR')
ActualvsPredict(y_test, test_pred_gam, 'GAM')
ActualvsPredict(y_test, test_pred_rf, 'RF')
ActualvsPredict(y_test, test_pred_svr, 'SVR')
ActualvsPredict(y_test, test_pred_xgb, 'XGBoost')
ActualvsPredict(y_test, test_pred_lgb, 'LGBM')
ActualvsPredict(y_test, test_pred_cat, 'CatBoost')
dev.off()
```



